{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LHBuilder/SA-Segment-Anything/blob/main/integ_yolo_sam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh8Mz39dDnje"
      },
      "source": [
        "Integrate YOLO-NAS and Meta SAM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ltYHi38rEyi-"
      },
      "source": [
        "Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ5SToe-EECh"
      },
      "outputs": [],
      "source": [
        "# py -3.10 -m venv myvenv\n",
        "# myvenv\\Scripts\\activate\n",
        "\n",
        "# !pip install super-gradients==3.1.0\n",
        "# !pip install imutils\n",
        "# !pip install pytube --upgrade\n",
        "\n",
        "# !pip install git+https://github.com/facebookresearch/segment-anything.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWf00XGxFMw4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.__version__\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lrL8Z5IFsFh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VCBmgJDF5JF"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "  if random_color:\n",
        "    color = np.concatenate([np.random.random(3), np.array([0.6])], aixs=0)\n",
        "  else:\n",
        "    color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "  h, w = mask.shape[-2:]\n",
        "  mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "  ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "  pos_points = coords[labels==1]\n",
        "  neg_points = coords[labels==0]\n",
        "  ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=market_size, edgecolor='white', linewidth=1.25)\n",
        "  ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=market_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "  x0, y0 = box[0], box[1]\n",
        "  w, h = box[2] - box[0], box[3] - box[1]\n",
        "  ax.add_patch(plt.Rectangle(x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
        "\n",
        "def show_anns(anns):\n",
        "  if len(anns) == 0:\n",
        "    return\n",
        "  sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_autoscale_on(False)\n",
        "  polygons = []\n",
        "  color = []\n",
        "  for ann in sorted_anns:\n",
        "    m = ann['segmentation']\n",
        "    img = np.ones(m.shape[0], m.shape[1], 3))\n",
        "    color_mask = np.random.random((1, 3)).tolist()[0]\n",
        "    for i in range(3):\n",
        "      img[:,:,i] = color_mask[i]\n",
        "    ax.imshow(np.dstack((img, m*0.35)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me33Hku6R2MQ"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread('images/person.jpg')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OJk54uLPSjZZ"
      },
      "source": [
        "**YOLO-NAS Detects Objects**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T1Gz2VbSp0R"
      },
      "outputs": [],
      "source": [
        "from super_gradients.training import models as yolon\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "image_path = 'images/person.jpg'\n",
        "\n",
        "model = yolon.get('yolo_nas_l', pretrained_wrights='coco') # yolo_nas_l is the yolo_nas large model\n",
        "model.predict(image, conf=0.25).show()\n",
        "\n",
        "conf_threshold = 0.25\n",
        "detection_pred = model.predict(image_path, conf=conf_threshold)\n",
        "detections = detection_pred.save('output_folder') # save the output with detected bounding box"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vRoH1DAeUVOT"
      },
      "source": [
        "**SAM Selects Objects**\n",
        "\n",
        "First, load the SAM model and predictor. Change the path below to point to the SAM checkpoint. And then run on CUDA and use the default model for best results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3g0jkvGVn8G"
      },
      "outputs": [],
      "source": [
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image = cv2.imread('images/person.jpg')\n",
        "\n",
        "# SAM model for masking\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#predictor = SamPredicator(sam)\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "masks = mask_generator.generate(image)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image)\n",
        "show_anns(masks)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wnXhO3b8GAW0"
      },
      "source": [
        "Now all the objects were masked but we only want to put mask on Person\n",
        "Steps:\n",
        "1. Object detection using YOLO-NAS\n",
        "2. Provide bounding box coordinates to SAM\n",
        "3. SAM will provide the mask on Person"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2nS-HQ8Gsmx"
      },
      "source": [
        "YOLO-NAS inference: Extract confidence, labels, and bounding boxes\n",
        "Access this information via the _images_prediction_lst attribute of the prediction objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGhU6j6mHBcy"
      },
      "outputs": [],
      "source": [
        "from super_gradients.training import models as yolon\n",
        "import cv2\n",
        "\n",
        "image_path = 'images/person.jpg'\n",
        "model = yolon.get(\"yolo_nas_l\", pretrained_weights=\"coco\")\n",
        "conf_threshold = 0.25\n",
        "\n",
        "detection_pred = model.predict(image_path, conf = conf_threshold)._images_prediction_lst"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "htRLHFzqH-XF"
      },
      "source": [
        "Extract only the desired information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYc7bUo2Hykz"
      },
      "outputs": [],
      "source": [
        "# Extract desired outputs\n",
        "bboxes_xyxy = detection_pred[0].prediction.bboxes_xyxy.tolist()\n",
        "confidence = detection_pred[0].prediction.confidence.tolist()\n",
        "labels = detection_pred[0].prediction.labels.tolist()\n",
        "\n",
        "print(\"Bounding Boxes (xyxy):\", bboxes_xyxy)\n",
        "print(\"Confidence:\", confidence)\n",
        "print(\"Labels:\", labels)\n",
        "\n",
        "bboxes_xyxy,confidence,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-v9iyGCItFa"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from super_gradients.training import models as yolon\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "image_path = 'images/person.jpg'\n",
        "image = cv2.imread('images/person.jpg')\n",
        "\n",
        "# SAM model for masking\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "device = \"cuba\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "predictor.set_image(image)\n",
        "\n",
        "image = image.transpose((2, 0, 1)) # Tranpose to match SAM input format\n",
        "image = image / 255.0 # Normalize image values to [0, 1]\n",
        "image = np.expand_dims(image, axis=0) # Add batch dimension\n",
        "\n",
        "input_box = np.array(bboxes_xyxy[0])\n",
        "\n",
        "if labels[0] == 0:\n",
        "  # predict masks using SAM\n",
        "  masks, _, _ = predictor.predict(\n",
        "      point_coords=None,\n",
        "      point_labels=None,\n",
        "      box=input_box[Noe, :],\n",
        "      multimask_output=False,\n",
        "  )\n",
        "\n",
        "# Display the image with masks and bounding box\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "if labels[0] == 0:\n",
        "  plt.imshow(masks[0], alpha=0.5)\n",
        "plt.gca().add_patch(plt.Rectangle((input_box[0], input_box[1]), input_box[2] - input_box[0], input_box[3] - input_box[1],\n",
        "                                   linewidth=2, edgecolor='r', facecolor='none'))\n",
        "plt.text(input_box[0], input_box[1] - 5, 'Person', fontsize=12, color='r', backgroundcolor='w')\n",
        "plt.axis('off')\n",
        "plt.savefig('output_folder/output3.png')\n",
        "plt.show"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPdX9neHHgbKM3QKmEnTJcf",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
